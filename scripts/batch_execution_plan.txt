===============================================================================
BATCH EMBEDDING GENERATION - EXECUTION PLAN
===============================================================================

Project: RouteRM Chatbot - RAG System
Documents: 132 processed documents
Target: PostgreSQL with pgvector on Railway/Vercel
Date: 2025-10-31

===============================================================================
STRATEGY OVERVIEW
===============================================================================

Batch Size: 15 documents per batch
Total Batches: 9 batches
Estimated Time: 20-40 minutes
Estimated Cost: $0.10-0.20 (OpenAI embeddings)

Chunking:
  - Chunk size: 500 tokens
  - Overlap: 50 tokens
  - Expected chunks: 4,000-6,000 total

Database Strategy:
  - Atomic commits per batch
  - Rollback on failure
  - Resume from last successful batch

===============================================================================
PRE-FLIGHT CHECKLIST
===============================================================================

[ ] 1. Environment Variables Set
    export DATABASE_URL="postgresql://..."
    export OPENAI_API_KEY="sk-..."

[ ] 2. Dependencies Installed
    pip install openai psycopg2-binary

[ ] 3. Database Schema Deployed
    Tables: rag_documents, rag_chunks
    Extensions: pgvector

[ ] 4. Documents Ready
    Location: /Users/a21/routellm-chatbot/data/processed-kb/documents-ready/
    Count: 132 JSON files
    Content: Each has extractedText or content field

[ ] 5. Database Connection Verified
    psql $DATABASE_URL -c "SELECT 1;"

===============================================================================
EXECUTION PHASES
===============================================================================

PHASE 1: TEST RUN (5 documents)
--------------------------------
Purpose: Validate end-to-end pipeline before full run

Command:
  cd /Users/a21/routellm-chatbot/scripts
  python3 batch_embeddings_processor.py --batch-size 5 --reset-state

Expected Results:
  - 1 batch processed
  - 5 documents inserted
  - ~150-200 chunks created
  - $0.01-0.02 cost
  - Time: 2-3 minutes

Verification:
  psql $DATABASE_URL -c "SELECT COUNT(*) FROM rag_documents;"  # Should be 5
  psql $DATABASE_URL -c "SELECT COUNT(*) FROM rag_chunks;"     # Should be ~150-200
  ./verify_batch_progress.sh

Decision Point:
  ✅ Test passed → Proceed to Phase 2
  ❌ Test failed → Debug before continuing

-------------------------------------------------------------------------------

PHASE 2: FULL PROCESSING (All 132 documents)
---------------------------------------------
Purpose: Process all documents in optimized batches

Approach A: Single Run (Recommended for local/powerful servers)
----------------------------------------------------------------
Command:
  python3 batch_embeddings_processor.py --batch-size 15 --reset-state

Expected Results:
  - 9 batches processed
  - 132 documents inserted
  - ~4,000-6,000 chunks created
  - $0.10-0.20 cost
  - Time: 20-40 minutes

Monitoring:
  Terminal 1: tail -f batch_embeddings.log
  Terminal 2: watch -n 10 ./verify_batch_progress.sh

Approach B: Staged Run (Recommended for Railway with strict timeouts)
----------------------------------------------------------------------
Stage 1: Batches 1-3 (45 documents)
  python3 batch_embeddings_processor.py --batch-size 15 --reset-state
  Wait for completion (~10 minutes)
  Verify: cat .batch_progress.json | jq '.processed_documents'  # Should be 45

Stage 2: Batches 4-6 (45 documents)
  python3 batch_embeddings_processor.py --batch-size 15
  Wait for completion (~10 minutes)
  Verify: cat .batch_progress.json | jq '.processed_documents'  # Should be 90

Stage 3: Batches 7-9 (42 documents)
  python3 batch_embeddings_processor.py --batch-size 15
  Wait for completion (~10 minutes)
  Verify: cat .batch_progress.json | jq '.processed_documents'  # Should be 132

Benefits of Staged Approach:
  ✅ Survives Railway deployment timeouts
  ✅ Can be run across multiple sessions
  ✅ Lower memory footprint
  ✅ Easier to monitor and debug

-------------------------------------------------------------------------------

PHASE 3: VERIFICATION (After completion)
-----------------------------------------
Purpose: Ensure all data is correctly inserted and searchable

Verification Steps:

1. Check document count:
   psql $DATABASE_URL -c "SELECT COUNT(*) FROM rag_documents;"
   Expected: 132

2. Check chunk count:
   psql $DATABASE_URL -c "SELECT COUNT(*) FROM rag_chunks;"
   Expected: 4,000-6,000 (varies by document length)

3. Check embeddings exist:
   psql $DATABASE_URL -c "SELECT COUNT(*) FROM rag_chunks WHERE embedding IS NOT NULL;"
   Expected: Same as chunk count

4. Run test queries:
   psql $DATABASE_URL < test_vector_search.sql

5. Check category distribution:
   psql $DATABASE_URL -c "
     SELECT metadata->>'category', COUNT(*)
     FROM rag_chunks
     GROUP BY metadata->>'category'
     ORDER BY COUNT(*) DESC;
   "

6. Test vector similarity:
   psql $DATABASE_URL -c "
     SELECT text, 1 - (embedding <=> (SELECT embedding FROM rag_chunks LIMIT 1)) AS similarity
     FROM rag_chunks
     ORDER BY embedding <=> (SELECT embedding FROM rag_chunks LIMIT 1)
     LIMIT 5;
   "

7. Verify progress state:
   ./verify_batch_progress.sh

Decision Point:
  ✅ All checks passed → Proceed to Phase 4
  ❌ Issues found → Debug and re-run failed batches

-------------------------------------------------------------------------------

PHASE 4: INTEGRATION (Connect to RAG system)
---------------------------------------------
Purpose: Integrate embeddings into production chatbot

Steps:

1. Update RAG retrieval function to use PostgreSQL:
   - Replace in-memory search with database queries
   - Use search_similar_chunks() function
   - Add metadata filtering by category

2. Test retrieval with sample queries:
   - "What is the claim authorization process?"
   - "Show me GAF warranty information"
   - "What are the building codes for Maryland?"

3. Optimize query performance:
   - Ensure HNSW index is being used
   - Monitor query times (should be < 100ms)
   - Add caching for frequent queries

4. Deploy to production:
   - Update Railway/Vercel deployment
   - Verify DATABASE_URL is set in production
   - Test end-to-end in production

===============================================================================
BATCH BREAKDOWN (15 documents per batch)
===============================================================================

Batch 1:  Documents 1-15     | Est. Time: 3-5 min  | Est. Cost: $0.011
Batch 2:  Documents 16-30    | Est. Time: 3-5 min  | Est. Cost: $0.011
Batch 3:  Documents 31-45    | Est. Time: 3-5 min  | Est. Cost: $0.011
Batch 4:  Documents 46-60    | Est. Time: 3-5 min  | Est. Cost: $0.011
Batch 5:  Documents 61-75    | Est. Time: 3-5 min  | Est. Cost: $0.011
Batch 6:  Documents 76-90    | Est. Time: 3-5 min  | Est. Cost: $0.011
Batch 7:  Documents 91-105   | Est. Time: 3-5 min  | Est. Cost: $0.011
Batch 8:  Documents 106-120  | Est. Time: 3-5 min  | Est. Cost: $0.011
Batch 9:  Documents 121-132  | Est. Time: 2-4 min  | Est. Cost: $0.009

Total:    132 documents      | Total Time: 27-45 min | Total Cost: ~$0.10-0.15

===============================================================================
ROLLBACK PLAN (If something goes wrong)
===============================================================================

Scenario 1: Batch Fails Mid-Processing
---------------------------------------
Action:
  1. Check logs: tail -50 batch_embeddings.log
  2. Identify issue (API error, DB error, etc.)
  3. Fix issue (e.g., increase delay, fix connection)
  4. Re-run: python3 batch_embeddings_processor.py --batch-size 15
  5. Script automatically resumes from last successful batch

Scenario 2: Out of Memory (Railway)
------------------------------------
Action:
  1. Reduce batch size: --batch-size 5
  2. Process in smaller stages
  3. Monitor memory usage in Railway dashboard

Scenario 3: Database Connection Lost
-------------------------------------
Action:
  1. Verify DATABASE_URL is correct
  2. Check database is running (Railway/Vercel dashboard)
  3. Test connection: psql $DATABASE_URL -c "SELECT 1;"
  4. Re-run script (resumes automatically)

Scenario 4: Need to Start Over
-------------------------------
Action:
  1. Run: ./reset_batch_processing.sh
  2. Confirms deletion of all data
  3. Re-run: python3 batch_embeddings_processor.py --batch-size 15 --reset-state

===============================================================================
SUCCESS CRITERIA
===============================================================================

✅ All 132 documents processed
✅ All chunks have embeddings (no NULL values)
✅ Vector search returns relevant results
✅ Category filtering works
✅ Total cost under $0.25
✅ Processing time under 1 hour
✅ No failed documents in state file
✅ Database tables properly indexed
✅ Integration with RAG system successful

===============================================================================
MONITORING DASHBOARD (During Execution)
===============================================================================

Terminal 1: Real-time Logs
---------------------------
tail -f /Users/a21/routellm-chatbot/scripts/batch_embeddings.log

Terminal 2: Progress Updates
-----------------------------
watch -n 10 "cat .batch_progress.json | jq '{batch: .current_batch, processed: .processed_documents, total: .total_documents, cost: .total_cost_usd}'"

Terminal 3: Database Stats
---------------------------
watch -n 30 "psql \$DATABASE_URL -t -c 'SELECT COUNT(*) FROM rag_documents;' && psql \$DATABASE_URL -t -c 'SELECT COUNT(*) FROM rag_chunks;'"

===============================================================================
EXECUTION LOG (Fill in as you progress)
===============================================================================

Date Started: _________________
Time Started: _________________

Phase 1 - Test Run:
  [ ] Started at: __________
  [ ] Completed at: __________
  [ ] Documents processed: 5/5
  [ ] Chunks created: ______
  [ ] Cost: $_______
  [ ] Status: SUCCESS / FAILED
  [ ] Notes: _______________________________________________

Phase 2 - Full Processing:
  [ ] Started at: __________
  [ ] Batch 1: __________ (Status: _______)
  [ ] Batch 2: __________ (Status: _______)
  [ ] Batch 3: __________ (Status: _______)
  [ ] Batch 4: __________ (Status: _______)
  [ ] Batch 5: __________ (Status: _______)
  [ ] Batch 6: __________ (Status: _______)
  [ ] Batch 7: __________ (Status: _______)
  [ ] Batch 8: __________ (Status: _______)
  [ ] Batch 9: __________ (Status: _______)
  [ ] Completed at: __________
  [ ] Total documents: 132/132
  [ ] Total chunks: ______
  [ ] Total cost: $_______
  [ ] Status: SUCCESS / FAILED
  [ ] Notes: _______________________________________________

Phase 3 - Verification:
  [ ] All documents in DB: YES / NO
  [ ] All chunks have embeddings: YES / NO
  [ ] Vector search working: YES / NO
  [ ] Status: SUCCESS / FAILED
  [ ] Notes: _______________________________________________

Phase 4 - Integration:
  [ ] RAG system updated: YES / NO
  [ ] Test queries working: YES / NO
  [ ] Production deployed: YES / NO
  [ ] Status: SUCCESS / FAILED
  [ ] Notes: _______________________________________________

===============================================================================
FINAL SIGN-OFF
===============================================================================

Completed by: _________________
Date: _________________
Total Time: _________________
Total Cost: $_________________
Documents Processed: _______/132
Chunks Created: _______
Status: SUCCESS / PARTIAL / FAILED

Notes:
______________________________________________________________________
______________________________________________________________________
______________________________________________________________________

===============================================================================
END OF EXECUTION PLAN
===============================================================================
